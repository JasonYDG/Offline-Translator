# Offline-Translator (Powered by Seed-X)

A privacy-focused offline AI translation application built with Transformers and Tkinter, providing secure local translation without sending data to external servers.

![Application Screenshot](screenshot.png)

> **âš ï¸ IMPORTANT NOTICE**: This repository contains placeholder model files. You must download the actual model files (~14GB) before running the application. See [Installation](#-installation) section for details.

## âœ¨ Key Features

- ğŸ¤– **Local AI Translation** - Complete privacy protection with offline processing
- ğŸŒ **Multi-language Support** - Bidirectional translation between 8 languages (Chinese, English, Spanish, French, German, Japanese, Korean, Russian)
- âš¡ **GPU Acceleration** - Automatic GPU detection with CUDA support for faster translation
- ğŸ¨ **Modern Interface** - Clean, intuitive graphical user interface
- âŒ¨ï¸ **Keyboard Shortcuts** - Rich hotkey support for efficient workflow
- ğŸ“‹ **One-click Copy** - Easy result copying to clipboard
- ğŸ”„ **Quick Language Swap** - Instant source/target language switching
- ğŸ“ **Smart Text Limits** - 5000 character limit with helpful user guidance

## ğŸ”§ System Requirements

- **Python**: 3.8 or higher
- **PyTorch**: Latest stable version
- **Transformers**: Hugging Face transformers library
- **Accelerate**: For optimized model loading
- **Tkinter**: GUI framework (usually included with Python)

## ğŸ“¦ Installation

1. **Clone the repository:**
```bash
git clone https://github.com/your-username/offline-translator.git
cd offline-translator
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **âš ï¸ IMPORTANT: Download Model Files**
   
   **The repository contains placeholder model files. You MUST download the actual model before running the application.**
   
   - ğŸ“ Navigate to the `models/` directory
   - ğŸ“– Read the `README.txt` file for detailed download instructions
   - ğŸ”— Download the model files (~14GB) from the provided links
   - ğŸ“‹ Replace the placeholder `model.safetensors` (0KB) with the actual model file
   
   **Quick Setup:**
   ```bash
   # After cloning, check the models directory
   cd models/
   cat README.txt  # Read download instructions
   
   # Download model files from the provided links
   # Replace model.safetensors with the downloaded file (~14GB)
   ```

4. **Verify Installation:**
   - Ensure `model.safetensors` is approximately 14GB (not 0KB)
   - All JSON configuration files should contain actual data
   - Run the application to test: `python translator.py`

## ğŸš€ Usage

1. **Launch the application:**
```bash
python translator.py
```

2. **Setup process:**
   - Select your preferred device (GPU recommended for speed)
   - Click "Load Model" to initialize the translation engine
   - Choose source and target languages
   - Enter text (up to 5000 characters)
   - Click "Translate" or press `Ctrl+Enter`

## âŒ¨ï¸ Keyboard Shortcuts

| Shortcut | Action |
|----------|--------|
| `Ctrl+Enter` | Translate text |
| `Ctrl+L` | Load model |
| `Ctrl+Shift+C` | Copy translation result |
| `Ctrl+Shift+X` | Clear input text |
| `Ctrl+Shift+S` | Swap source and target languages |
| `F1` | Show help information |

## ğŸ–¥ï¸ Interface Features

### Device Selection
- **Automatic GPU Detection** - Scans for available CUDA devices
- **GPU Priority** - Graphics cards listed first, CPU as fallback
- **Device Information** - Shows GPU names for easy identification

### Language Management
- **8 Language Support** - Comprehensive language coverage
- **Bidirectional Translation** - Translate between any supported language pair
- **Quick Swap Button** (â‡„) - Instantly reverse translation direction
- **Smart Validation** - Prevents selecting identical source/target languages

### Text Processing
- **Scrollable Text Areas** - Handle long texts comfortably
- **5000 Character Limit** - Optimized for quality and performance
- **Clear Warnings** - Helpful messages when text exceeds limits
- **Copy/Clear Functions** - Easy text management

### Translation Optimization
- **Advanced Prompting** - Optimized prompts for better translation quality
- **Smart Output Cleaning** - Removes artifacts and explanatory text
- **Tuned Parameters** - Optimized generation settings for accuracy

## ğŸ“ Model Structure

Your `models/` directory should contain:

```
models/
â”œâ”€â”€ config.json              # Model configuration
â”œâ”€â”€ model.safetensors        # Model weights
â”œâ”€â”€ tokenizer.json           # Tokenizer configuration
â””â”€â”€ generation_config.json   # Generation parameters
```

## ğŸ”§ Technical Features

### Memory Optimization
- **Efficient Loading** - Uses `accelerate` library for optimized memory usage
- **Mixed Precision** - `bfloat16` support to reduce VRAM consumption
- **Smart Device Management** - Automatic device allocation and detection

### Error Handling
- **Global Exception Handling** - Comprehensive error catching
- **Detailed Error Messages** - Clear feedback for troubleshooting
- **Automatic Logging** - Errors saved to `error_log.txt`

### User Experience
- **Multi-threading** - Non-blocking UI during translation
- **Real-time Progress** - Live status updates and progress indicators
- **Responsive Design** - Smooth interaction and feedback

## ğŸ› ï¸ Troubleshooting

### Common Issues

**Model Loading Fails**
- Verify all model files are present in `models/` directory
- Check file permissions and integrity
- Ensure sufficient disk space

**CUDA Errors**
- Confirm PyTorch CUDA compatibility
- Update GPU drivers
- Check CUDA installation

**Memory Issues**
- Try CPU mode for large models
- Reduce input text length
- Close other GPU-intensive applications

**Poor Translation Quality**
- Verify model compatibility with selected language pair
- Check if model is specifically trained for translation
- Try shorter, simpler sentences

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ Support

If you encounter any issues or have questions, please open an issue on GitHub.